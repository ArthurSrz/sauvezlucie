---
title: Exploration vs exploitation dans l'apprentissage par renforcement
type: concept
tags:
- apprentissage par renforcement
- IA
- exploration
- exploitation
- algorithmes
- optimisation
- machine learning
- dilemme
- prise de décision
date_creation: '2025-03-28'
date_modification: '2025-03-29'
isPartOf:
- '[[Apprentissage par renforcement]]'
- '[[Algorithmes de bandit manchot et exploration-exploitation]]'
uses: '[[Systèmes de recommandation en IA]]'
hasPart: '[[Algorithmes de recherche heuristique en IA]]'
---
## Généralité

Le dilemme exploration-exploitation est un concept fondamental en [apprentissage par renforcement](https://fr.wikipedia.org/wiki/Apprentissage_par_renforcement) qui décrit le compromis entre l'exploration de nouvelles actions pour découvrir leurs récompenses potentielles et l'utilisation des connaissances déjà acquises pour maximiser les récompenses. Ce problème est également connu sous le nom de "problème du bandit manchot", en référence aux machines à sous des casinos où un joueur doit choisir entre plusieurs machines avec des probabilités de gain inconnues.

## Points clés

- **L'[exploitation](https://fr.wikipedia.org/wiki/Exploitation_(apprentissage_par_renforcement))** : Choisir les actions qui maximisent la récompense attendue selon les connaissances actuelles
- **L'[exploration](https://fr.wikipedia.org/wiki/Exploration_(apprentissage_par_renforcement))** : Tester de nouvelles actions pour découvrir de meilleures stratégies à long terme
- **Équilibre optimal** : Essentiel pour éviter les optima locaux et découvrir la politique optimale
- **Algorithmes clés** : ε-greedy, UCB, Softmax, Thompson sampling
- **Applications** : Systèmes de recommandation, publicité en ligne, robotique, jeux vidéo

## Détails

Un agent d'apprentissage par renforcement doit constamment équilibrer exploration et exploitation pour optimiser ses performances à long terme. Dans l'[apprentissage par renforcement](https://fr.wikipedia.org/wiki/Apprentissage_par_renforcement), un agent interagit avec un environnement en effectuant des actions et en recevant des récompenses. L'objectif est d'apprendre une politique optimale qui maximise la récompense cumulée sur le temps. Ce dilemme trouve des applications dans de nombreux domaines comme les [systèmes de recommandation](https://fr.wikipedia.org/wiki/Syst%C3%A8me_de_recommandation), la recherche médicale, ou l'optimisation des moteurs de recherche.

Plusieurs algorithmes ont été développés pour gérer ce compromis :
- **ε-greedy** : Explore aléatoirement avec une probabilité ε (typiquement entre 5% et 10%) tout en exploitant le meilleur choix connu le reste du temps
- **Softmax** : Sélectionne les actions selon une distribution de probabilité basée sur leurs récompenses estimées
- **[UCB (Upper Confidence Bound)](https://fr.wikipedia.org/wiki/Algorithme_Upper_Confidence_Bound)** : Utilise des bornes de confiance statistique pour sélectionner les actions dont la récompense potentielle est la plus incertaine
- **[Thompson sampling](https://fr.wikipedia.org/wiki/Thompson_sampling)** : Méthode bayésienne qui maintient une distribution de probabilité sur les récompenses potentielles

Ce dilemme se retrouve dans de nombreuses applications pratiques :
- **Systèmes de recommandation** : Plateformes comme Netflix ou Spotify utilisent des algorithmes ε-greedy ou Thompson sampling
- **Optimisation de publicités en ligne** : Plateformes comme Google Ads utilisent souvent l'algorithme UCB
- **Robotique adaptative** : Méthodes comme l'exploration basée sur l'entropie permettent aux robots d'apprendre dans des environnements inconnus
- **Jeux vidéo** : Les IA comme [AlphaGo](https://fr.wikipedia.org/wiki/AlphaGo) utilisent des combinaisons sophistiquées d'exploration et d'exploitation

Le choix de la stratégie dépend des caractéristiques du problème, comme la stationnarité de l'environnement et les contraintes temporelles. Dans les environnements non-stationnaires (comme les marchés financiers), une exploration continue est généralement nécessaire.