---
title: Algorithmes de bandit manchot et exploration-exploitation
type: concept
tags:
- apprentissage par renforcement
- bandit manchot
- exploration-exploitation
- algorithmes
- prise de décision
- optimisation
- machine learning
- incertitude
date_creation: '2025-03-29'
date_modification: '2025-03-29'
hasPart: '[[Exploration vs exploitation dans l''apprentissage par renforcement]]'
subClassOf: '[[Apprentissage par renforcement]]'
uses: '[[Systèmes de recommandation en IA]]'
---
## Généralité

Les algorithmes de bandit manchot (multi-armed bandit) constituent une classe de problèmes d'apprentissage par renforcement où un agent doit décider quelle action choisir parmi plusieurs options, chacune offrant une récompense incertaine. Le nom vient de l'analogie avec les machines à sous (bandits manchots) dans un casino. Le dilemme fondamental de ces algorithmes est l'équilibre entre l'exploration (tester différentes options pour acquérir de l'information) et l'exploitation (choisir l'option qui semble la plus prometteuse selon les connaissances actuelles).

## Points clés

- Le problème du bandit manchot modélise la prise de décision séquentielle sous incertitude avec un compromis entre exploration et exploitation
- Les stratégies courantes incluent ε-greedy, UCB (Upper Confidence Bound), Thompson Sampling et Softmax
- Ces algorithmes sont largement utilisés dans les systèmes de recommandation, les tests A/B, l'optimisation de publicités en ligne et l'apprentissage automatique
- La performance est généralement mesurée par le regret cumulé, qui quantifie la différence entre les récompenses obtenues et celles qui auraient pu être obtenues avec la meilleure action

## Détails

Dans un problème de bandit manchot classique, un agent fait face à K "bras" (actions possibles), chacun associé à une distribution de probabilité inconnue qui détermine les récompenses. À chaque itération, l'agent sélectionne un bras, observe la récompense obtenue, et utilise cette information pour améliorer sa stratégie future.

Le dilemme exploration-exploitation est au cœur de ces algorithmes. L'exploration consiste à essayer différentes actions pour en apprendre davantage sur leurs distributions de récompenses. L'exploitation consiste à choisir l'action qui semble optimale selon les informations déjà recueillies. Trop d'exploration peut entraîner des pertes inutiles, tandis que trop d'exploitation peut empêcher la découverte d'actions potentiellement meilleures.

Plusieurs stratégies ont été développées pour résoudre ce dilemme:

1. **ε-greedy**: Avec une probabilité ε, l'agent explore en choisissant une action aléatoire; sinon, il exploite en choisissant l'action avec la récompense moyenne la plus élevée observée jusqu'à présent.

2. **UCB (Upper Confidence Bound)**: Cette approche sélectionne l'action qui maximise une borne supérieure de confiance, combinant la récompense moyenne observée et un terme d'exploration qui diminue à mesure que l'action est choisie.

3. **Thompson Sampling**: Une approche bayésienne qui maintient une distribution de probabilité sur les récompenses potentielles de chaque action et échantillonne à partir de ces distributions pour faire des choix.

4. **Softmax/Boltzmann**: Attribue des probabilités de sélection à chaque action proportionnellement à leurs récompenses moyennes estimées, avec un paramètre de température contrôlant le degré d'exploration.

Les applications des algorithmes de bandit manchot sont nombreuses. Dans les systèmes de recommandation, ils permettent d'équilibrer la suggestion d'articles populaires (exploitation) avec la découverte des préférences de l'utilisateur (exploration). Pour les tests A/B, ils peuvent optimiser dynamiquement l'allocation du trafic vers les variantes les plus performantes. Dans la publicité en ligne, ils aident à sélectionner les annonces qui maximisent le taux de clics tout en explorant de nouvelles options.

Les extensions modernes incluent les bandits contextuels (qui prennent en compte des informations supplémentaires pour chaque décision) et les bandits non stationnaires (où les distributions de récompenses changent au fil du temps).