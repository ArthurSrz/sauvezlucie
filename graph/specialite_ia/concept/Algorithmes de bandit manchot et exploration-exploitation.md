---
title: Algorithmes de bandit manchot et exploration-exploitation
type: concept
tags:
- apprentissage par renforcement
- bandit manchot
- exploration-exploitation
- algorithmes
- prise de décision
- optimisation
- machine learning
- incertitude
date_creation: '2025-03-29'
date_modification: '2025-03-29'
hasPart: '[[Exploration vs exploitation dans l''apprentissage par renforcement]]'
subClassOf: '[[Apprentissage par renforcement]]'
uses: '[[Systèmes de recommandation en IA]]'
---
## Généralité

Les algorithmes de [bandit manchot](https://fr.wikipedia.org/wiki/Probl%C3%A8me_du_bandit_manchot) (multi-armed bandit) constituent une classe de problèmes d'[apprentissage par renforcement](https://fr.wikipedia.org/wiki/Apprentissage_par_renforcement) où un agent doit décider quelle action choisir parmi plusieurs options, chacune offrant une récompense incertaine. Le nom vient de l'analogie avec les machines à sous (bandits manchots) dans un casino, où un joueur doit choisir entre plusieurs machines avec des probabilités de gain différentes mais inconnues.

Le problème a été formalisé pour la première fois par [Herbert Robbins](https://fr.wikipedia.org/wiki/Herbert_Robbins) en 1952 et modélise la prise de décision séquentielle sous incertitude avec un compromis fondamental entre exploration et exploitation.

## Points clés

- **Dilemme exploration-exploitation**: Équilibre entre tester de nouvelles options (exploration) et choisir les options connues les plus prometteuses (exploitation)
- **Applications variées**: Optimisation publicitaire, recommandation de contenu, essais cliniques, recherche scientifique
- **Algorithmes principaux**: ε-greedy, UCB (Upper Confidence Bound), [Thompson Sampling](https://fr.wikipedia.org/wiki/%C3%89chantillonnage_de_Thompson)
- **Mesure de performance**: Regret cumulé (différence entre récompenses obtenues et meilleure stratégie possible)
- **Extensions modernes**: Bandits contextuels (avec informations supplémentaires) et bandits non stationnaires (distributions changeantes)

## Détails

Dans un problème classique de [bandit manchot](https://fr.wikipedia.org/wiki/Bandit_manchot_(math%C3%A9matiques)), un agent fait face à K "bras" (actions possibles), chacun associé à une distribution de probabilité inconnue qui détermine les récompenses. À chaque itération, l'agent sélectionne un bras, observe la récompense obtenue, met à jour ses connaissances, et répète le processus pour maximiser les récompenses cumulées.

Les principaux algorithmes incluent :
- **ε-greedy** qui alterne entre exploration aléatoire (avec probabilité ε) et exploitation du meilleur bras connu
- **UCB (Upper Confidence Bound)** qui sélectionne l'action maximisant une borne supérieure de confiance
- **Thompson Sampling**, une approche bayésienne qui maintient des distributions de probabilité sur les récompenses
- **Softmax/Boltzmann** où les probabilités de sélection sont proportionnelles aux récompenses estimées

Les applications pratiques sont nombreuses :
- Publicité en ligne pour optimiser le taux de clics
- Systèmes de recommandation pour équilibrer contenus populaires et découverte
- Tests A/B avec allocation dynamique du trafic
- Essais cliniques en médecine
- Sélection d'expériences scientifiques à mener

La performance est mesurée par le regret cumulé (différence avec la meilleure stratégie possible). Les algorithmes optimaux atteignent un regret en O(√T) pour récompenses arbitraires ou O(log T) pour récompenses sous-gaussiennes. Les extensions modernes incluent les bandits contextuels (prenant en compte des informations supplémentaires) et les bandits non stationnaires (adaptés aux distributions changeantes).