---
title: Exploration vs exploitation dans l'apprentissage par renforcement
type: concept
tags:
- apprentissage par renforcement
- IA
- exploration
- exploitation
- algorithmes
- optimisation
- machine learning
- dilemme
- prise de décision
date_creation: '2025-03-22'
date_modification: '2025-03-22'
isPartOf: '[[Apprentissage par renforcement:]]'
---
## Généralité

Le dilemme exploration-exploitation est un concept fondamental en apprentissage par renforcement qui décrit le compromis entre l'exploration de nouvelles actions pour découvrir leurs récompenses potentielles (exploration) et l'utilisation des connaissances déjà acquises pour maximiser les récompenses (exploitation). Un agent d'apprentissage par renforcement doit constamment équilibrer ces deux stratégies pour optimiser ses performances à long terme.

## Points clés

- L'exploitation consiste à choisir les actions qui, selon les connaissances actuelles, maximisent la récompense attendue
- L'exploration implique de tester de nouvelles actions, potentiellement sous-optimales à court terme, pour découvrir de meilleures stratégies à long terme
- Un équilibre optimal entre exploration et exploitation est essentiel pour éviter les optima locaux et découvrir la politique optimale
- Différentes stratégies comme ε-greedy, softmax, UCB (Upper Confidence Bound) et Thompson sampling permettent de gérer ce compromis

## Détails

Dans l'apprentissage par renforcement, un agent interagit avec un environnement en effectuant des actions et en recevant des récompenses. L'objectif est d'apprendre une politique optimale qui maximise la récompense cumulée sur le temps.

L'exploitation pure consiste à toujours choisir l'action qui semble la meilleure selon les connaissances actuelles. Cette stratégie peut être efficace si l'agent dispose déjà d'informations précises sur l'environnement, mais elle risque de rester bloquée dans des optima locaux si certaines actions potentiellement meilleures n'ont jamais été essayées.

À l'inverse, l'exploration pure implique de choisir des actions aléatoirement ou systématiquement, indépendamment des récompenses attendues. Cette approche garantit la découverte de toutes les possibilités mais peut être inefficace car elle ne tire pas parti des connaissances acquises.

Plusieurs méthodes permettent de gérer ce compromis :

1. **Stratégie ε-greedy** : L'agent choisit l'action optimale avec une probabilité (1-ε) et une action aléatoire avec une probabilité ε. Le paramètre ε peut être réduit progressivement (décroissance de ε) pour favoriser l'exploitation au fil du temps.

2. **Méthode softmax** : Les actions sont sélectionnées selon une distribution de probabilité basée sur leurs valeurs estimées, modulée par un paramètre de température qui contrôle le degré d'exploration.

3. **Upper Confidence Bound (UCB)** : Cette approche ajoute un bonus d'exploration aux actions moins fréquemment testées, favorisant ainsi l'exploration des actions incertaines.

4. **Thompson sampling** : Utilise une approche bayésienne en échantillonnant à partir de distributions de probabilité sur les valeurs des actions.

Le choix de la stratégie d'exploration-exploitation dépend souvent des caractéristiques du problème, comme la stationnarité de l'environnement, la taille de l'espace d'états-actions, et les contraintes temporelles. Dans les environnements non-stationnaires, où les récompenses associées aux actions changent au fil du temps, une exploration continue est généralement nécessaire pour détecter ces changements.

## Applications pratiques

Ce dilemme se retrouve dans de nombreuses applications comme les systèmes de recommandation, l'optimisation de publicités en ligne, la robotique adaptative et les jeux. Par exemple, un algorithme de recommandation doit équilibrer la suggestion d'articles que l'utilisateur appréciera probablement (exploitation) avec la proposition de nouveaux types de contenu pour affiner ses préférences (exploration).